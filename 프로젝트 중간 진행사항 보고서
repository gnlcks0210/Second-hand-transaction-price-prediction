📰 네이버 뉴스 트렌드 분석 프로젝트 (11주차)

프로젝트 주제: "인공지능" 키워드에 대한 네이버 뉴스 데이터를 수집, 분석, 시각화하고, 머신러닝(비지도 학습)을 적용하여 핵심 토픽을 도출합니다.

1. 🎯 프로젝트 개요

프로젝트 목표:

"인공지능" 키워드 관련 뉴스 데이터를 네이버 API를 통해 안정적으로 수집합니다.

pandas와 KoNLPy를 사용해 데이터를 정제하고 텍스트 분석을 위한 전처리를 수행합니다.

Matplotlib, Seaborn, WordCloud로 트렌드와 핵심 키워드를 시각화합니다.

scikit-learn의 K-Means 클러스터링(비지도 학습)을 적용하여 기사들을 주제별로 자동 분류하고, 숨겨진 하위 토픽을 분석합니다.

프로젝트 범위 (교안 기준):

데이터 수집-크롤링 → 데이터 가공/정제 → 데이터 분석 → 데이터 시각화

(데이터 분석 단계에 머신러닝 포함)

2. 🚀 현재까지 진행 상황 (11주차)

프로젝트 계획에 따라 1, 2단계를 완료하여 머신러닝 분석을 위한 최종 데이터셋을 확보했습니다.

2-1. 환경 구축 (Google Colab)

konlpy 라이브러리 사용을 위한 Java(OpenJDK 11) 환경을 구축했습니다.

이슈 해결: 교안의 Java 8 버전 사용 시, 최신 konlpy 및 jpype 라이브러리와의 버전 충돌로 Java Fatal Error(SIGSEGV)가 발생했습니다. (제출한 hs_err_pid.log 파일 참고) Java 11로 버전을 변경하여 이 문제를 해결했습니다.

분석에 필요한 pandas, konlpy, matplotlib, seaborn 등 핵심 라이브러리를 설치했습니다.

2-2. 1단계: 데이터 수집 (Crawling)

수집 방식: BeautifulSoup을 이용한 직접 스크래핑 방식 대신, 교안 2주차에서 배운 Naver 검색 API를 활용했습니다. [cite: 1914-1993]

변경 사유: 네이버의 잦은 HTML 구조 변경으로 인한 크롤링 실패 문제를 근본적으로 해결하고, 더 안정적으로 데이터를 확보하기 위함입니다.

수집 실행: "인공지능" 키워드로 뉴스 기사 약 500건을 수집하여 1차 naver_news.csv 파일로 저장했습니다.

수집 항목: title (제목), press (언론사 원문 링크), date (날짜), description (요약), url (네이버 링크)

2-3. 2단계: 데이터 가공 및 정제 (Preprocessing)

교안 3, 4주차에서 배운 pandas 라이브러리를 활용하여 naver_news.csv 파일을 정제했습니다.

데이터 클리닝 (교안 4주차):

isnull().sum()으로 결측치를 확인하고 dropna()로 제거했습니다. [cite: 18176-18181, 18323-18329]

duplicated(subset=['title'])로 중복 기사를 확인하고 drop_duplicates()로 제거했습니다. [cite: 18453-18457, 18475-18481]

데이터 변환 (교안 6주차):

pd.to_datetime() 함수를 사용해 'date' 열을 분석 가능한 날짜(datetime) 형식으로 변환했습니다. [cite: 16183]

텍스트 전처리 (교안 2, 10주차):

KoNLPy (Okt) 형태소 분석기를 사용했습니다.

re.sub()로 기사 제목(title)에서 한글, 영어, 숫자를 제외한 모든 특수문자를 제거했습니다.

okt.nouns()를 사용해 각 제목에서 명사만 추출했습니다. [cite: 5813-5844]

'수', '것', '등'과 같이 의미 없는 1글자 명사를 제거하고, apply 함수를 이용해 결과를 'keywords'라는 새 열에 저장했습니다. (교안 6주차 참고) [cite: 15956-15962]

3. 💾 현재 산출물

naver_news_refined.csv

위의 모든 수집 및 정제 과정이 완료된 최종 데이터셋입니다.

'keywords' 열에 각 기사의 핵심 명사 리스트가 포함되어 즉시 분석 가능합니다.

데이터 예시 (df.head())

(실제 크롤링 및 정제 완료 시의 예상 출력입니다.)

title

date

keywords

press

description

url

인공지능이 여는 미래, 10년 뒤 직업은

2025-11-14

['인공지능', '미래', '직업']

http://www.etnews.com...

AI 기술이 발전함에 따라 10년 뒤...

http://news.naver.com...

AI 반도체 전쟁, 삼성전자 승부수

2025-11-14

['반도체', '전쟁', '삼성전자', '승부수']

http://www.chosun.com...

글로벌 AI 반도체 시장을 선점하기 위한...

http://news.naver.com...

정부, 챗GPT 윤리 가이드라인 발표

2025-11-13

['정부', '챗GPT', '윤리', '가이드라인', '발표']

http://www.yna.co.kr...

정부가 급속도로 발전하는 생성형 AI...

http://news.naver.com...

의료계에 부는 인공지능 바람

2025-11-13

['의료계', '인공지능', '바람']

http://www.donga.com...

AI를 활용한 신약 개발 및 영상 진단...

http://news.naver.com...

(이하 485개 행...)

...

[...]

...

...

...

4. 🗺️ 향후 계획 (기말 프로젝트까지)

머신러닝 요건을 충족하기 위해, 3단계(시각화)와 4단계(머신러닝)를 다음과 같이 진행할 계획입니다.

3단계: 데이터 분석 및 시각화 (교안 10, 11주차)

시계열 분석: date 열을 기준으로 일별 기사 건수를 집계하고, seaborn.lineplot을 사용해 트렌드 변화를 시각화합니다. [cite: 5125-5132, 9088-9092]

키워드 빈도 분석: keywords 열의 모든 단어를 취합하여 가장 많이 언급된 핵심 키워드를 WordCloud로 시각화합니다. (교안 2주차 참고) [cite: 2974-2986]

언론사 분석: press 열을 기준으로 언론사별 기사 수를 집계하고 seaborn.barplot으로 시각화합니다. [cite: 5157-5160, 9152-9155]

4단계: 머신러닝 분석 적용 (교안 7, 9주차)

(비지도 학습) K-Means 클러스터링 (군집화):

scikit-learn의 TfidfVectorizer를 사용해 keywords 데이터를 머신러닝이 이해할 수 있는 숫자 벡터(TF-IDF 행렬)로 변환합니다.

교안 7, 9주차의 K-Means 알고리즘 [cite: 13058-13082, 15648-15830, 16488-16512, 17208-17255]을 적용하여 "인공지능" 기사들을 자동으로 토픽(주제)별로 그룹화합니다.

결과 분석: 각 그룹(클러스터)에서 가장 빈도가 높은 키워드를 확인하여, "인공지능"의 하위 주제(e.g., 'AI 반도체', 'AI 윤리', 'AI 의료' 등)를 도출합니다.
